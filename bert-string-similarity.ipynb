{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vertify - String Similarity with BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAYHSZFgIWZl2uzA99DaAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihaiaperghis/python-seo/blob/main/bert-string-similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRTwGOjuupE"
      },
      "source": [
        "# Using pre-trained BERT models and Google Sheets to compare string similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-czTODK2tD7"
      },
      "source": [
        "## Description\n",
        "\n",
        "The goal of this notebook is to use pre-trained BERT models for comparing a set of string to another set of strings, taking each string from the first set in order to find the most similar one from the second set.\n",
        "\n",
        "This can prove to be **highly useful for SEO**, such as for **mapping keywords to landing pages** or for **complex site migrations**, where comparing title tags, H1s, or other text scraped via crawlers can be compared in terms of similarity when no other type of automated matching (such as via URLs, SKUs, etc.) can be done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ywDSneVv4kq"
      },
      "source": [
        "## Required packages\n",
        "\n",
        "We use the [Sentence Transformers framework](https://www.sbert.net/index.html) for our pre-trained BERT models. It allows us to load any BERT model from their library or via [HuggingFace](https://huggingface.co/models) and makes it very easy to calculate semantic similarity between various pieces of text.\n",
        "\n",
        "We also use the [gspread](https://docs.gspread.org/) library to interact with our Google Spreadsheets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUICG29wUMi1"
      },
      "source": [
        "# Install Sentence Transformers and upgrade gspread\n",
        "!pip install --quiet sentence-transformers\n",
        "!pip install -U --quiet gspread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0_AQlVYzkdG"
      },
      "source": [
        "## Load the BERT model\n",
        "\n",
        "When loading a model directly from the Sentence Transormers library, we should use one that is [optimized for detecting text similarity](https://www.sbert.net/docs/pretrained_models.html#semantic-textual-similarity). In this case, loading the model is extremely straightforward, just do\n",
        "\n",
        "```\n",
        "model = SentenceTransformer('model name')\n",
        "```\n",
        "\n",
        "If looking for a non-English model, we might get better results by using a custom language-trained model from the [HuggingFace library](https://huggingface.co/models) instead of the multilingual ones. In that case, we need a few extra lines of code for getting the BERT model and adapting it for use with sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsn4xQMdUgFn"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "# Load a pre-trained BERT model for mapping tokens to embeddings.\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# If you'd like to use one of the pre-trained models from the HuggingFace library, use the code below instead.\n",
        "# word_embedding_model = models.Transformer('dumitrescustefan/bert-base-romanian-uncased-v1')\n",
        "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "#                                pooling_mode_mean_tokens=True,\n",
        "#                                pooling_mode_cls_token=False,\n",
        "#                                pooling_mode_max_tokens=False)\n",
        "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FU7jQu1tal"
      },
      "source": [
        "## Authenticate with Google\n",
        "\n",
        "Since we'll be pulling/pushing data from/to Google Sheets, we need to authenticate the script with our Google account that has access to our spreadsheet. Click on the link that the script generates, copy the code and paste it in the box you'll see below the link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5G-lnRti8Vd"
      },
      "source": [
        "# Authenticate with Google and access API services for Google Sheets\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8unYoER2LQK"
      },
      "source": [
        "## Get the data from our spreadsheet\n",
        "\n",
        "We go through multiple steps in order to fetch both lists of strings (in our case these could be title tags, headings, any other text that we might have from our crawls or other sources):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVnmrbrr4Eot"
      },
      "source": [
        "### Establish stopwords\n",
        "\n",
        "If we're mainly comparing short strings (such as title tags or headings), it makes sense to remove stopwords from both sets of strings in order to make sure we're comparing 'apples to apples' (especially with title tags, a lot of SEO optimizations involve removing stopwords). We're also adding a few other non-alphanumeric symbols for removal, since it's likely they'll add more confusion than clarity to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU36NRrJ3rl5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get stopwords and add some symbols we want to remove\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words = stop_words + ['<', '%', ':', '&']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYVzsT0J4_Os"
      },
      "source": [
        "### Load and clean up the data \n",
        "\n",
        "Assuming we have a separate sheet with the data that represents the first set of strings and one with the data featuring the second set, we use the gspread module to load everything from our Google Sheets spreadsheet based on what columns we're interested in. We need to use numerical numbers for our columns, so, for example, column E would be number 5.\n",
        "\n",
        "The rest of the code is to:\n",
        "\n",
        "* Remove the header (using .pop)\n",
        "* Make all words lowercase\n",
        "* Remove the stopwords established earlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnwFacC8VY9e"
      },
      "source": [
        "import gspread\n",
        "\n",
        "# Where to find the data in Google Sheets\n",
        "spreadsheet_id = '***************************************'\n",
        "sheet_first_set = '******' # Name of first sheet\n",
        "sheet_first_set_column = 1 # Column A\n",
        "sheet_second_set = '******' # Name of second sheet\n",
        "sheet_second_set_column = 1 # Column A\n",
        "\n",
        "# Fetch the data from the spreadsheet, remove header row\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "ss = gc.open_by_key(spreadsheet_id)\n",
        "first_set_strings = ss.worksheet(sheet_first_set).col_values(sheet_first_set_column)\n",
        "first_set_strings.pop(0)\n",
        "second_set_strings = ss.worksheet(sheet_second_set).col_values(sheet_second_set_column)\n",
        "second_set_strings.pop(0)\n",
        "\n",
        "# Make everything lowercase and remove stopwords\n",
        "first_set_strings_clean = first_set_strings.copy()\n",
        "second_set_strings_clean = second_set_strings.copy()\n",
        "\n",
        "for idx, string in enumerate(first_set_strings_clean):\n",
        "  string = [word.lower() for word in string.split(' ') if word not in stop_words]\n",
        "  string =  ' '.join(string)\n",
        "  first_set_strings_clean[idx] = string\n",
        "\n",
        "for idx, string in enumerate(second_set_strings_clean):\n",
        "  string = [word.lower() for word in string.split(' ') if word not in stop_words]\n",
        "  string = ' '.join(string)\n",
        "  second_set_strings_clean[idx] = string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSA9cIto-7Pl"
      },
      "source": [
        "### Print the results\n",
        "\n",
        "This is just a 'sanity' check to make sure all of our changes have been applied correctly: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrNMklu_T7z"
      },
      "source": [
        "print(f\"First Set Strings: \\t{first_set_strings}\")\n",
        "print(f\"Clean First Set Strings: \\t{first_set_strings_clean}\")\n",
        "print(f\"Second Set Strings: \\t{second_set_strings}\")\n",
        "print(f\"Clean Second Set Strings: \\t{second_set_strings_clean}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYgk9tc7_OiA"
      },
      "source": [
        "## Calculate string similarity\n",
        "\n",
        "This is where we use the BERT model we previously loaded to calculate vectors for each string from first set and compare to all of the strings from the second set. The results are scores from 0 to 1 that reflect the [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity), 1 meaning two strings are basically identical from a semantic point of view.\n",
        "\n",
        "Once the calculations are done, the results are sorted descending by this score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4QQc7DAnWo"
      },
      "source": [
        "### Test the similarity calculations\n",
        "\n",
        "Before actually using the values returned by the model, let's first print out a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo-UBFA4VsoU"
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Get embeddings from BERT for all our strings\n",
        "first_set_embeddings = model.encode(first_set_strings_clean)\n",
        "second_set_embeddings = model.encode(second_set_strings_clean)\n",
        "\n",
        "# Compare the 'distance' (similarity) between each of our first set and the second set\n",
        "for i, (first_set_string, first_set_string_clean, first_set_embedding) in \\\n",
        "  enumerate(zip(first_set_strings, first_set_strings_clean, first_set_embeddings)):\n",
        "    \n",
        "    distances = cdist([first_set_embedding], second_set_embeddings, \"cosine\")[0]\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "    \n",
        "    # Print the top 5 new titles for each old titles\n",
        "    print(f\"First Set String: {first_set_string}\\n\\n\")\n",
        "    print(\"Second Set Similarity:\\n\")\n",
        "    \n",
        "    for j, (idx, distance) in enumerate(results):\n",
        "        print(f\"{second_set_strings[idx]} (Score: {(1-distance):.2f})\")\n",
        "        if (j + 1) % 5 == 0:\n",
        "          break\n",
        "    print(\"\\n-------------------------------\\n\")\n",
        "    if (i + 1) % 20 == 0:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTHlqVOTCHq8"
      },
      "source": [
        "### Load the calculations in a dataframe\n",
        "\n",
        "We basically repeat the above code but this time we load the results in a Pandas dataframe (basically a table) that contains the top two 'predictions' (highest similarity scores) for each string from the first set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w5ZjEIFHmv2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set up a  dataframe that holds our calculations\n",
        "df_model = pd.DataFrame(columns=['First Set String', 'String Similarity 1', 'Similarity Score 1', 'String Similarity 2', 'Similarity Score 2'])\n",
        "df_model.set_index('First Set String', inplace = True)\n",
        "\n",
        "# Compare the 'distance' (similarity) between each of our first set and the second set\n",
        "for i, (first_set_string, first_set_string_clean, first_set_embedding) in \\\n",
        "  enumerate(zip(first_set_strings, first_set_strings_clean, first_set_embeddings)):\n",
        "    \n",
        "    distances = cdist([first_set_embedding], second_set_embeddings, \"cosine\")[0]\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    # Add top 2 predictions to the dataframe\n",
        "    for j, (idx, distance) in enumerate(results):\n",
        "      if (j == 0):    \n",
        "        df_model.at[first_set_string, 'String Similarity 1'] = second_set_strings[idx]\n",
        "        df_model.at[first_set_string, 'Similarity Score 1'] = '%.2f' % (1-distance)\n",
        "      if (j == 1): \n",
        "        df_model.at[first_set_string, 'String Similarity 2'] = second_set_strings[idx]\n",
        "        df_model.at[first_set_string, 'Similarity Score 2'] = '%.2f' % (1-distance)\n",
        "\n",
        "df_model.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWpSHgq2C8Yf"
      },
      "source": [
        "## Upload the results back in the spreadsheet\n",
        "\n",
        "All we're doing here is refine the table to match the order of rows from our spreadsheet, so our predictions are sorted correctly. Afterwards, we simply add a few headers and upload the data into the columns of choice from the sheet with our first set of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeRKyP4jyJM-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set up the final dataframe that matches the order of rows from the spreadsheet\n",
        "df_final = pd.DataFrame(first_set_strings, columns = ['First Set String'])\n",
        "df_final.set_index('First Set String', inplace = True)\n",
        "\n",
        "# Merge similarity calculations in the new dataframe to sort them for the spreadsheet\n",
        "df = df_final.merge(df_model, how='left', on=['First Set String']) \n",
        "df.fillna('', inplace=True)\n",
        "\n",
        "# Write similar strings and their scores to our spreadsheet (first sheet)\n",
        "ss.worksheet(sheet_first_set).update('B1:B', np.array([['String Similarity 1'] + df['String Similarity 1'].tolist()]).T.tolist())\n",
        "ss.worksheet(sheet_first_set).update('C1:C', np.array([['Similarity Score 1'] + df['Similarity Score 1'].tolist()]).T.tolist())\n",
        "ss.worksheet(sheet_first_set).update('D1:D', np.array([['String Similarity 2'] + df['String Similarity 2'].tolist()]).T.tolist())\n",
        "ss.worksheet(sheet_first_set).update('E1:E', np.array([['Similarity Score 2'] + df['Similarity Score 2'].tolist()]).T.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}